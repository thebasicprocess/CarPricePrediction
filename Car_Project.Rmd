---
title: "RitProject_Project"
  author: "Suhil/Mohammed/Shaika/Mahra"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

```{r}
# === Data Understanding ===

## ==== Step 0: Load Necessary Libraries ====
cat("\n==== Step 0: Load Necessary Libraries ====\n")

# List of all required libraries
required_libraries <- c(
  "tidyr",         # Data tidying
  "ggplot2",       # Visualization
  "dplyr",         # Data manipulation
  "caret",         # Machine learning and feature selection
  "randomForest",  # Random Forest implementation
  "scales",        # Scaling and formatting in plots
  "reshape2",      # Data reshaping
  "glue",          # String interpolation
  "moments",       # Skewness and kurtosis calculations
  "data.table",    # Efficient data handling
  "RColorBrewer",  # Color palettes
  "patchwork"      # Combining ggplot objects
)

# Function to check, install, and load libraries
load_library <- function(package) {
  if (!requireNamespace(package, quietly = TRUE)) {  # Check if the package is installed
    cat(sprintf("Installing missing library: %s\n", package))
    install.packages(package, dependencies = TRUE)  # Install the package if missing
  }
  library(package, character.only = TRUE)  # Load the package
}

# Apply the function to all required libraries
invisible(lapply(required_libraries, load_library))

cat("All required libraries are loaded.\n")

# --- Step 1: Load Dataset ---
cat("\n==== Step 1: Load Dataset ====\n")

data <- tryCatch(
  {
    read.csv("car_price.csv")
  },
  error = function(e) {
    stop("Error loading dataset: ", e$message)
  }
)

cat("Dataset loaded successfully.\n")

# --- Step 2: Overview of Loaded Dataset ---
cat("\n==== Step 2: Overview of Loaded Dataset ====\n")

# Dataset dimensions
cat("\n---- Dataset Overview ----\n")
cat("Dataset Dimensions (Rows x Columns): ", dim(data), "\n")
cat("Column Names:\n", paste(names(data), collapse = ", "), "\n")

# Calculate and print dataset size in memory
data_size <- object.size(data)
cat("Approximate Data Size in Memory: ", format(data_size, units = "auto"), "\n")

# Data types and structure
cat("\n---- Data Types and Structure ----\n")
print(str(data))

# --- Step 3: Convert Data Types ---
cat("\n==== Step 3: Convert IDs and Categorical Columns to Appropriate Types ====\n")

data$car_ID <- as.factor(data$car_ID)  # Assuming car_ID is an identifier
data$fueltype <- as.factor(data$fueltype)
data$aspiration <- as.factor(data$aspiration)
data$doornumber <- as.factor(data$doornumber)
data$carbody <- as.factor(data$carbody)
data$drivewheel <- as.factor(data$drivewheel)
data$enginelocation <- as.factor(data$enginelocation)
data$enginetype <- as.factor(data$enginetype)
data$cylindernumber <- as.factor(data$cylindernumber)
data$fuelsystem <- as.factor(data$fuelsystem)

cat("Data types converted successfully.\n")

# --- Step 4: Profile the Dataset ---
cat("\n==== Step 4: Profile the Dataset ====\n")

# Summary statistics
cat("\n---- Summary Statistics ----\n")
numeric_cols <- setdiff(names(data)[sapply(data, is.numeric)], "car_ID")  # Exclude car_ID
categorical_cols <- setdiff(names(data)[sapply(data, is.factor)], "car_ID")  # Exclude car_ID

cat("\n--- Summary Statistics for Numeric Columns ---\n")
print(summary(data[numeric_cols]))

cat("\n--- Summary Statistics for Categorical Columns ---\n")
print(summary(data[categorical_cols]))

# --- Step 5: Visualize Numeric Variables ---
cat("\n==== Step 5: Visualize Numeric Variables ====\n")

plot_numeric_variable <- function(data, variable) {
  cat(glue::glue("\nDisplaying histogram for numeric variable: {variable}\n"))
  
  # Histogram
  print(
    ggplot(data, aes_string(x = variable)) +
      geom_histogram(fill = "lightblue", color = "black", bins = 30) +
      labs(title = paste("Histogram for", variable), x = variable, y = "Frequency") +
      theme_minimal()
  )
}

if (length(numeric_cols) > 0) {
  lapply(numeric_cols, function(col) plot_numeric_variable(data, col))
}

# --- Step 6: Visualize Categorical Variables ---
cat("\n==== Step 6: Visualize Categorical Variables ====\n")

plot_categorical_variable <- function(data, variable) {
  cat(glue::glue("\nDisplaying barplot for categorical variable: {variable}\n"))
  
  print(
    ggplot(data, aes_string(x = variable)) +
      geom_bar(fill = "skyblue", color = "darkblue") +
      labs(title = paste("Barplot for", variable), x = variable, y = "Count") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  )
}

if (length(categorical_cols) > 0) {
  lapply(categorical_cols, function(col) plot_categorical_variable(data, col))
}

# ---  Reflection on Dataset Profiling ---
cat("\n====  Reflection on Dataset Profiling ====\n")

cat(glue("
  1. **Dataset Size and Variables**:
     - Rows: {dim(data)[1]}, Columns: {dim(data)[2]}.
     - Approximate memory size: {format(data_size, units = 'auto')}.

  2. **Variable Visualizations**:
     - Histograms displayed for numeric variables (excluding car_ID).
     - Bar plots displayed for categorical variables (excluding car_ID).

  Profiling helps identify preprocessing needs, such as outlier handling, scaling, and encoding categorical features.
"))


# === Data Preparation ===

# --- Step 1: Missing Values ---

# --- Sub-step 1.1: Identify Missing Values ---
cat("\n---- Sub-step 1.1: Identifying Missing Values ----\n")

# Calculate the number of missing values in each column
missing_values <- colSums(is.na(data))
cat("Missing values in each column:\n")
print(missing_values)

# Visualize Missing Values Before Handling
cat("\nVisualizing missing values (before handling)...\n")
missing_df <- data.frame(Column = names(missing_values), MissingCount = missing_values)
missing_df <- missing_df[missing_df$MissingCount > 0, ]  # Filter columns with missing values
if (nrow(missing_df) > 0) {
  # Bar plot for missing values before handling
  missing_plot_before <- ggplot(missing_df, aes(x = reorder(Column, -MissingCount), y = MissingCount)) +
    geom_bar(stat = "identity", fill = "skyblue", color = "darkblue") +
    labs(
      title = "Missing Values Per Column (Before Handling)",
      x = "Column",
      y = "Number of Missing Values"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  print(missing_plot_before)
} else {
  cat("No missing values found in the dataset.\n")
}

# --- Sub-step 1.2: Handle Missing Values in car_ID ---
cat("\n---- Sub-step 1.2: Handling Missing Values in car_ID ----\n")

# car_ID is a unique identifier, so rows with missing car_ID values should be removed.
if (anyNA(data$car_ID)) {
  cat("Handling missing values in car_ID: Removing rows with missing car_ID values.\n")
  data <- data[!is.na(data$car_ID), ]  # Remove rows where car_ID is NA
  cat("Rows with missing car_ID values removed.\n")
} else {
  cat("No missing values found in car_ID.\n")
}

# --- Sub-step 1.3: Handle Numeric Missing Values (Median Imputation) ---
cat("\n---- Sub-step 1.3: Handling Numeric Missing Values ----\n")

# Identify numeric columns
numeric_cols <- names(data)[sapply(data, is.numeric)]

# Loop through each numeric column and fill missing values with the median
for (col in numeric_cols) {
  if (sum(is.na(data[[col]])) > 0) {
    median_val <- median(data[[col]], na.rm = TRUE)  # Calculate the median excluding NAs
    data[[col]][is.na(data[[col]])] <- median_val  # Replace missing values with the median
    cat(sprintf("Filled missing values in numeric column '%s' with median: %.2f\n", col, median_val))
  }
}

# --- Sub-step 1.4: Handle Categorical Missing Values (Mode Imputation) ---
cat("\n---- Sub-step 1.4: Handling Categorical Missing Values ----\n")

# Identify categorical columns (character or factor)
categorical_cols <- names(data)[sapply(data, function(x) is.character(x) || is.factor(x))]

# Loop through each categorical column and fill missing values with the mode
for (col in categorical_cols) {
  if (sum(is.na(data[[col]])) > 0) {
    mode_val <- names(which.max(table(data[[col]], useNA = "no")))  # Find the mode
    data[[col]][is.na(data[[col]])] <- mode_val  # Replace missing values with the mode
    cat(sprintf("Filled missing values in categorical column '%s' with mode: '%s'\n", col, mode_val))
  }
}

# --- Sub-step 1.5: Verify and Visualize Post-Imputation ---
cat("\n---- Sub-step 1.5: Verifying Post-Imputation ----\n")

# Recalculate the number of missing values in each column
final_missing_values <- colSums(is.na(data))
cat("Remaining missing values in each column (should be 0):\n")
print(final_missing_values)

# Visualize Missing Values After Handling
cat("\nVisualizing missing values (after handling)...\n")
final_missing_df <- data.frame(Column = names(final_missing_values), MissingCount = final_missing_values)
final_missing_df <- final_missing_df[final_missing_df$MissingCount > 0, ]
if (nrow(final_missing_df) > 0) {
  # Bar plot for missing values after handling
  missing_plot_after <- ggplot(final_missing_df, aes(x = reorder(Column, -MissingCount), y = MissingCount)) +
    geom_bar(stat = "identity", fill = "lightgreen", color = "darkgreen") +
    labs(
      title = "Missing Values Per Column (After Handling)",
      x = "Column",
      y = "Number of Missing Values"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  print(missing_plot_after)
} else {
  cat("All missing values have been successfully handled.\n")
}

# --- Reflection on Step 1: Missing Values ---
cat("\n==== Reflection on Step 1: Missing Values ====\n")
cat(
  "1. **Identification**:\n",
  "- Missing values were identified and visualized, allowing us to assess the extent of the issue.\n\n",
  
  "2. **Imputation Strategies**:\n",
  "- `car_ID`: Rows with missing values were removed as it is a unique identifier.\n",
  "- Numeric columns: Median imputation was used to ensure robustness to outliers.\n",
  "- Categorical columns: Mode imputation preserved the most common category.\n\n",
  
  "3. **Post-Imputation Analysis**:\n",
  "- All columns were rechecked to ensure no residual missing values.\n",
  "- Visualizations confirmed the success of the missing value handling process.\n\n",
  
  "By addressing missing values systematically, the dataset is now complete and ready for further preprocessing or modeling.\n"
)

# --- Step 2: Handle Duplicates ---
cat("\n==== Step 2: Handle Duplicates ====\n")

# Check for and remove duplicate rows
num_duplicates <- nrow(data) - nrow(dplyr::distinct(data))
cat(sprintf("Number of duplicate rows: %d\n", num_duplicates))

# Remove duplicates
data <- dplyr::distinct(data)
cat(sprintf("Number of rows after removing duplicates: %d\n", nrow(data)))

# --- Reflection on Step 2: Handling Duplicates ---
cat("\n==== Reflection on Step 2: Handling Duplicates ====\n")
cat(
  "1. **Initial Check**:\n",
  "- Identified and reported the number of duplicate rows: ", num_duplicates, ".\n\n",
  
  "2. **Handling**:\n",
  "- Removed all duplicate rows using `dplyr::distinct()`.\n",
  "- Remaining rows after duplicate removal: ", nrow(data), ".\n\n",
  
  "By addressing duplicates, the dataset is now free of redundant rows, ensuring consistency and accuracy for subsequent preprocessing.\n"
)

# --- Step 3: Handle Outliers ---
cat("\n==== Step 3: Handle Outliers ====\n")

# --- Sub-step 3.1: Identify Outliers ---
cat("\n---- Sub-step 3.1: Identifying Outliers in Numerical Columns ----\n")

# Function to calculate IQR bounds
identify_outliers <- function(column) {
  Q1 <- quantile(column, 0.25, na.rm = TRUE)
  Q3 <- quantile(column, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  list(lower = Q1 - 1.5 * IQR, upper = Q3 + 1.5 * IQR)
}

# Identify numeric columns
numeric_cols <- names(data)[sapply(data, is.numeric)]

# Store original data for comparison
data_original <- data

# --- Sub-step 3.2: Handle Outliers Using Capping ---
cat("\n---- Sub-step 3.2: Handling Outliers Using Capping ----\n")

# Loop through numeric columns to cap outliers
for (col in numeric_cols) {
  bounds <- identify_outliers(data[[col]])
  outliers <- which(data[[col]] < bounds$lower | data[[col]] > bounds$upper)
  if (length(outliers) > 0) {
    data[[col]][outliers] <- pmin(pmax(data[[col]][outliers], bounds$lower), bounds$upper)
    cat(sprintf("Column: %-15s | Outliers Adjusted: %-4d | Bounds: [%.2f, %.2f]\n", 
                col, length(outliers), bounds$lower, bounds$upper))
  } else {
    cat(sprintf("Column: %-15s | Outliers: None\n", col))
  }
}

# --- Sub-step 3.3: Compare Before and After Handling ---
cat("\n---- Sub-step 3.3: Comparing Before and After Outlier Handling ----\n")

for (col in numeric_cols) {
  plot <- ggplot() +
    geom_boxplot(data = data_original, aes(x = "Original Data", y = .data[[col]]), 
                 outlier.colour = "red", fill = "lightblue", color = "darkblue") +
    geom_boxplot(data = data, aes(x = "After Capping", y = .data[[col]]), 
                 outlier.colour = "red", fill = "lightgreen", color = "darkblue") +
    labs(title = paste("Comparison of", col, "- Before and After Outlier Handling"), x = "", y = col) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 14)) +
    scale_y_continuous(labels = scales::comma)
  print(plot)
}

# --- Why Use Capping for Predicting Car Prices? ---
cat("\n---- Why Use Capping for Predicting Car Prices? ----\n")
cat(
  "Capping reduces the impact of outliers while retaining rare but valid cases (e.g., luxury cars),\n",
  "ensuring the model learns from all data without losing critical information.\n"
)

# --- Reflection on Step 3: Handling Outliers ---
cat("\n==== Reflection on Step 3: Handling Outliers ====\n")
cat(
  "1. Outliers were detected using the IQR method and visualized with boxplots.\n",
  "2. Capping limited extreme values' influence while preserving data integrity.\n",
  "3. Boxplots showed the effectiveness of capping in reducing outlier impact.\n",
  "This ensures the dataset is robust and ready for analysis.\n"
)
# --- Step 4: Scaling / Normalizing Features ---
# 
# Scaling involves normalizing numeric features to a consistent range, often between 0 and 1.
# This step is critical for machine learning algorithms sensitive to feature magnitude, such as:
#   - Gradient Descent-Based Models (e.g., Linear or Logistic Regression)
#   - K-Nearest Neighbors (KNN)
#   - Support Vector Machines (SVM)
#   - Neural Networks
# 
# Min-Max Scaling transforms each feature using the formula:
#   Scaled Value = (Value - Min) / (Max - Min)
# This ensures that all features contribute equally during model training.
# 
# 
# # Identify numeric columns
# numeric_cols <- names(data)[sapply(data, is.numeric)]
# 
# # Apply Min-Max Scaling to Numeric Columns
# cat("\n---- Scaling Numeric Columns ----\n")
# scaled_data <- data  # Create a copy of the dataset for scaling
# scaled_data[numeric_cols] <- lapply(data[numeric_cols], function(col) {
#   scaled_col <- (col - min(col, na.rm = TRUE)) / (max(col, na.rm = TRUE) - min(col, na.rm = TRUE))
#   return(scaled_col)
# })
# cat("Features scaled using Min-Max scaling.\n")
# 
# # Optional: Visualize Original vs. Scaled Data
# cat("\n---- Visualizing Original vs. Scaled Data (Optional) ----\n")
# for (col in numeric_cols) {
#   plot <- ggplot() +
#     # Original data distribution
#     geom_density(data = data, aes_string(x = col), fill = "lightblue", alpha = 0.5, color = "darkblue") +
#     # Scaled data distribution
#     geom_density(data = scaled_data, aes_string(x = col), fill = "lightgreen", alpha = 0.5, color = "darkgreen") +
#     labs(
#       title = paste("Comparison of", col, "- Original vs. Scaled"),
#       x = col,
#       y = "Density"
#     ) +
#     theme_minimal() +
#     theme(
#       plot.title = element_text(hjust = 0.5, face = "bold", size = 14)
#     )
#   print(plot)
# }

# --- Reflection on Step 4: Scaling / Normalizing Features ---
cat("\n==== Reflection on Step 4: Scaling / Normalizing Features ====\n")
cat(
  "1. Scaling was skipped because the selected machine learning algorithms (e.g., Random Forest, Gradient Boosting)\n",
  "   are tree-based models that do not rely on feature scaling.\n",
  "2. These models split data based on thresholds, making scaling unnecessary.\n",
  "3. Skipping scaling ensures computational efficiency without compromising model performance.\n",
  "4. If future models require distance-based methods (e.g., KNN or SVM), scaling can be revisited.\n"
)

# --- Step 5: Data Transformation ---
cat("\n==== Step 5: Data Transformation ====\n")

# Identify numeric columns
numeric_cols <- names(data)[sapply(data, is.numeric)]

# Function to calculate skewness
library(moments)  # Ensure the 'moments' package is installed
calculate_skewness <- function(column) {
  skewness(column, na.rm = TRUE)
}

# Apply Log Transformation to Highly Skewed Numeric Columns
cat("\n---- Applying Log Transformation to Highly Skewed Columns ----\n")
transformed_cols <- c()
for (col in numeric_cols) {
  skew_val <- calculate_skewness(data[[col]])  # Calculate skewness
  cat(sprintf("Skewness of '%s': %.2f\n", col, skew_val))
  
  if ((skew_val > 0.5 || skew_val < -0.5) && min(data[[col]], na.rm = TRUE) > 0) {  # Adjusted threshold
    data[[paste0(col, "_log")]] <- log(data[[col]])
    transformed_cols <- c(transformed_cols, col)
    cat(sprintf("Applied log transformation to '%s' (Skewness: %.2f).\n", col, skew_val))
  } else {
    cat(sprintf("Skipped log transformation for '%s' (Skewness: %.2f).\n", col, skew_val))
  }
}

# --- Visualize Original vs. Log Transformed Data ---
cat("\n---- Visualizing Original vs. Log Transformed Data ----\n")
for (col in transformed_cols) {
  log_col <- paste0(col, "_log")
  if (log_col %in% names(data)) {  # Ensure log-transformed column exists
    plot <- ggplot() +
      geom_density(data = data, aes_string(x = col), fill = "lightblue", alpha = 0.5, color = "darkblue") +
      geom_density(data = data, aes_string(x = log_col), fill = "lightgreen", alpha = 0.5, color = "darkgreen") +
      labs(
        title = paste("Comparison of", col, "- Original vs. Log Transformed"),
        x = col,
        y = "Density"
      ) +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, face = "bold", size = 14)
      )
    print(plot)
  }
}

# --- Reflection on Step 5: Data Transformation ---
cat("\n==== Reflection on Step 5: Data Transformation ====\n")
cat(
  "1. Log transformation was applied to numeric columns with moderate skewness (Skewness > 0.5 or < -0.5) and positive values.\n",
  "2. This helps in reducing skewness, stabilizing variance, and potentially improving linear relationships for better model performance.\n",
  "3. Columns with low skewness or non-positive values were skipped to avoid unnecessary transformations or errors.\n",
  "4. Visualization confirmed the effectiveness of log transformation in normalizing distributions.\n"
)

# === Step 6: Feature Engineering ===
cat("\n==== Step 6: Feature Engineering ====\n")

# --- Sub-step 6.1: Review Existing Features ---
cat("\n---- Reviewing Existing Features ----\n")

# Display column names and summary statistics
cat("Current column names:\n")
print(names(data))
cat("\nSummary statistics:\n")
print(summary(data))

# --- Sub-step 6.2: Feature Engineering ---
cat("\n---- Engineering Features  ----\n")

# 1. **Interaction Features: Horsepower-to-Weight Ratio**
cat("\nCalculating Horsepower-to-Weight Ratio...\n")
if ("horsepower" %in% names(data) && "curbweight" %in% names(data)) {
  data$hp_to_weight <- data$horsepower / data$curbweight
  cat("Horsepower-to-weight ratio created.\n")
} else {
  cat("Skipped horsepower-to-weight ratio due to missing 'horsepower' or 'curbweight'.\n")
}

# 2. **Vehicle Segment Classification (Luxury, Standard, Economy)**
cat("\nClassifying Vehicle Segments...\n")
if ("price" %in% names(data)) {
  quantiles <- quantile(data$price, probs = c(0.33, 0.66, 1.0), na.rm = TRUE)
  data$segment <- cut(
    data$price,
    breaks = c(-Inf, quantiles[1], quantiles[2], quantiles[3]),
    labels = c("Economy", "Standard", "Luxury"),
    include.lowest = TRUE
  )
  cat("Vehicle segments classified into Economy, Standard, and Luxury.\n")
} else {
  cat("Skipped vehicle segment classification due to missing 'price'.\n")
}

# 3. **Fuel Economy Score**
cat("\nCalculating Fuel Economy Score...\n")
if ("citympg" %in% names(data) && "highwaympg" %in% names(data)) {
  data$fuel_economy <- (data$citympg + data$highwaympg) / 2
  cat("Fuel economy score created.\n")
} else {
  cat("Skipped fuel economy score due to missing 'citympg' or 'highwaympg'.\n")
}

# 4. **Brand Value**
cat("\nCreating Brand Value Feature...\n")
if ("CarName" %in% names(data)) {
  data <- data %>%
    mutate(CarBrand = tolower(gsub(" .*", "", CarName)))  # Extract brand name from CarName
  
  # Standardize brand names
  brand_corrections <- c("maxda" = "mazda", "vw" = "volkswagen", "vokswagen" = "volkswagen", 
                         "porcshce" = "porsche", "toyouta" = "toyota")
  data$CarBrand <- recode(data$CarBrand, !!!brand_corrections)
  
  # Calculate average price by brand
  brand_avg_price <- data %>%
    group_by(CarBrand) %>%
    summarise(BrandAvgPrice = mean(price, na.rm = TRUE), .groups = "drop")
  data <- data %>%
    left_join(brand_avg_price, by = "CarBrand")
  cat("Brand value feature created.\n")
} else {
  cat("Skipped brand value due to missing 'CarName'.\n")
}


# --- Sub-step 6.3: Visualizing Features ---
cat("\n---- Visualizing New Features ----\n")

library(ggplot2)

# 1. Visualize Horsepower-to-Weight Ratio
if ("hp_to_weight" %in% names(data)) {
  ggplot(data, aes(x = hp_to_weight, y = price)) +
    geom_point(color = "firebrick", alpha = 0.7) +  
    labs(
      title = "Horsepower-to-Weight Ratio vs Price",
      x = "Horsepower-to-Weight Ratio",
      y = "Price"
    ) +
    theme_minimal() +
    theme(plot.title = element_text(color = "darkred", size = 14)) %>%
    print()
}

# 2. Visualize Vehicle Segment Distribution
if ("segment" %in% names(data)) {
  ggplot(data, aes(x = segment, fill = segment)) +
    geom_bar(color = "black", fill = c("#66c2a5", "#fc8d62", "#8da0cb")) +  
    labs(
      title = "Vehicle Segment Distribution",
      x = "Segment",
      y = "Count"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(color = "darkblue", size = 14),
      legend.position = "none"
    ) %>%
    print()
}

# 3. Visualize Fuel Economy Score
if ("fuel_economy" %in% names(data)) {
  ggplot(data, aes(x = fuel_economy, y = price)) +
    geom_point(color = "darkgreen", alpha = 0.7) +  
    labs(
      title = "Fuel Economy Score vs Price",
      x = "Fuel Economy Score",
      y = "Price"
    ) +
    theme_minimal() +
    theme(plot.title = element_text(color = "forestgreen", size = 14)) %>%
    print()
}

# 4. Visualize Brand Value (Average Price by Brand)
if ("BrandAvgPrice" %in% names(data)) {
  ggplot(data, aes(x = reorder(CarBrand, price, median), y = price, fill = CarBrand)) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.7) +  # Boxplot with outliers highlighted
    labs(
      title = "Price Distribution by Car Brand",
      x = "Car Brand",
      y = "Price"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(color = "darkblue", size = 14),
      axis.text.x = element_text(angle = 45, hjust = 1)
    ) +
    scale_fill_manual(values = rainbow(length(unique(data$CarBrand))))  # Custom rainbow palette for brands
} else {
  cat("BrandAvgPrice or CarBrand is not available for visualization.\n")
}

# --- Reflection on Step 6: Feature Engineering ---
cat("\n==== Reflection on Step 6: Feature Engineering ====\n")
cat(
  "1. Horsepower-to-Weight Ratio:\n",
  "   - Captures the relationship between engine power and vehicle weight.\n",
  "   - Indicates vehicle performance, relevant for pricing decisions.\n",
  "2. Vehicle Segments:\n",
  "   - Classified cars into Economy, Standard, and Luxury based on price quantiles.\n",
  "   - Adds domain knowledge about market positioning of vehicles.\n",
  "3. Fuel Economy Score:\n",
  "   - Combines city and highway MPG into a single efficiency metric.\n",
  "   - Provides insights into fuel consumption trends.\n",
  "4. Brand Value:\n",
  "   - Created 'BrandAvgPrice' to reflect consumer perception and reliability for each brand.\n",
  "These features ensure linear relationships, interpretability, and improved model performance for linear regression.\n"
)
# === Step 7: Visualizing Features Against Price ===
cat("\n==== Step 7: Visualizing Features Against Price ====\n")

library(ggplot2)
library(dplyr)
library(scales)

# --- Sub-step 7.1: Boxplots for Categorical Variables Against Price ---
cat("\n---- Boxplots for Categorical Variables Against Price ----\n")

# Identify categorical columns
categorical_cols <- names(data)[sapply(data, is.character)]
cat("Categorical Columns Identified:\n")
print(categorical_cols)

# Clean data for categorical boxplots
data_cleaned <- data %>%
  filter(!is.na(price)) %>%
  filter(!apply(., 1, function(row) any(is.na(row) | row == "")))

if (nrow(data_cleaned) == 0) {
  stop("Error: The dataset 'data_cleaned' is empty after removing null or empty values.")
}

# Generate boxplots for categorical variables
for (col in categorical_cols) {
  if (col == "CarName") {
    # Handle 'CarName': Visualize top 10 most frequent car names
    cat(sprintf("\nColumn '%s' has many categories. Visualizing top 10 most frequent.\n", col))
    
    # Get the top 10 most frequent CarNames
    top_cars <- data_cleaned %>%
      count(CarName) %>%
      arrange(desc(n)) %>%
      slice(1:10)
    
    # Filter data to include only top 10 CarNames
    filtered_data <- data_cleaned %>% filter(CarName %in% top_cars$CarName)
    
    # Create the boxplot
    plot <- ggplot(filtered_data, aes(x = reorder(CarName, price, FUN = median), y = price, fill = CarName)) +
      geom_boxplot(outlier.colour = "gold", outlier.shape = 16, outlier.size = 3, alpha = 0.8) +
      labs(
        title = "Price Distribution by Top 10 Most Frequent Car Names",
        x = "Car Name",
        y = "Price"
      ) +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16, color = "purple"),
        axis.text.x = element_text(angle = 45, hjust = 1, color = "darkblue"),
        axis.text.y = element_text(color = "darkgreen")
      ) +
      scale_y_continuous(labels = scales::comma) +
      scale_fill_manual(values = grDevices::rainbow(10))  # Dynamic palette for top 10 categories
    
    # Print the plot
    print(plot)
  } else {
    # Standard boxplots for other categorical variables
    num_colors <- length(unique(data_cleaned[[col]]))
    plot <- ggplot(data_cleaned, aes_string(x = col, y = "price", fill = col)) +
      geom_boxplot(outlier.colour = "gold", outlier.shape = 16, outlier.size = 3, alpha = 0.8) +
      labs(
        title = paste("Price Distribution by", col),
        x = col,
        y = "Price"
      ) +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16, color = "purple"),
        axis.text.x = element_text(angle = 45, hjust = 1, color = "darkblue"),
        axis.text.y = element_text(color = "darkgreen")
      ) +
      scale_y_continuous(labels = scales::comma) +
      scale_fill_manual(values = grDevices::rainbow(num_colors))  # Dynamic palette for all categories
    
    # Print the plot
    print(plot)
  }
}

cat("\n---- Why Boxplots for Categorical Variables? ----\n")
cat(
  "Boxplots help visualize price distributions across categories, highlighting medians, variability, and outliers. 
  For 'CarName', only the top 10 most frequent categories are visualized for better interpretability.\n"
)

# --- Sub-step 7.2: Scatterplots for Numerical Variables Against Price ---
cat("\n---- Scatterplots for Numerical Variables Against Price ----\n")

# Identify numerical columns
numeric_cols <- names(data_cleaned)[sapply(data_cleaned, is.numeric)]
cat("Numerical Columns Identified:\n")
print(numeric_cols)

# Generate scatterplots for numerical variables
for (col in numeric_cols) {
  if (col != "price") {
    cat(sprintf("\nCreating scatterplot for '%s' vs 'price'.\n", col))
    
    # Dynamically generate a large enough color palette for CarName
    num_colors <- length(unique(data_cleaned$CarName))
    color_palette <- grDevices::rainbow(num_colors)
    
    plot <- ggplot(data_cleaned, aes_string(x = col, y = "price")) +
      geom_point(aes(color = CarName), size = 2.5, alpha = 0.6) +  # Adjusted size and transparency for points
      geom_smooth(method = "lm", color = "red", se = FALSE, linewidth = 1) +  # Trendline for correlation
      labs(
        title = paste("Relationship Between", col, "and Price"),
        x = col,
        y = "Price"
      ) +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16, color = "darkred"),
        axis.text.x = element_text(size = 10, color = "darkblue"),
        axis.text.y = element_text(size = 10, color = "darkgreen"),
        legend.position = "none"  # Remove legend for simplicity
      ) +
      scale_y_continuous(labels = scales::comma) +
      scale_x_continuous(labels = scales::comma) +
      scale_color_manual(values = color_palette)  # Apply dynamic color palette
    
    # Print the plot
    print(plot)
  }
}

cat("\n---- Why Scatterplots for Numerical Variables? ----\n")
cat(
  "Scatterplots reveal relationships, trends, and correlations between numerical variables and price. 
  Including a linear trendline helps identify significant predictors for car pricing.\n"
)

# === Step 8: Redundant Features and Feature Selection ===
cat("\n==== Step 8: Redundant Features and Feature Selection ====\n")

library(caret)  # For findCorrelation function
library(ggplot2)  # For heatmap visualization
library(dplyr)  # For data manipulation
library(reshape2)  # For reshaping correlation matrix

# --- Sub-step 8.1: Check and Remove Redundant Features ---
cat("\n---- Checking and Removing Redundant Features ----\n")

# Identify and remove explicitly redundant columns
redundant_cols <- c("Car_ID")  # Replace with known redundant column names
if (all(redundant_cols %in% names(data))) {
  data <- data %>% select(-all_of(redundant_cols))
  cat("Removed redundant features:\n")
  print(redundant_cols)
} else {
  cat("No specified redundant features found.\n")
}

# --- Sub-step 8.2: Feature Selection Based on Correlation Matrix ---
cat("\n---- Feature Selection Based on Correlation Matrix ----\n")

# Identify numeric columns
numeric_cols <- names(data)[sapply(data, is.numeric)]
cat("Numeric Columns Identified:\n")
print(numeric_cols)

# Ensure there are numeric columns
if (length(numeric_cols) == 0) {
  stop("Error: No numeric columns found in the dataset.")
}

# Generate the correlation matrix for numeric columns
correlation_matrix <- cor(data[numeric_cols], use = "complete.obs")

# Reshape the correlation matrix for visualization
correlation_melted <- melt(correlation_matrix)
colnames(correlation_melted) <- c("Feature1", "Feature2", "Correlation")

# Create a heatmap of the correlation matrix
heatmap_plot <- ggplot(correlation_melted, aes(x = Feature1, y = Feature2, fill = Correlation)) +
  geom_tile(color = "white") +  # Add gridlines to tiles
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", midpoint = 0, 
    limit = c(-1, 1), name = "Correlation"  # Define color scale limits and legend title
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10, color = "darkblue"),
    axis.text.y = element_text(size = 10, color = "darkblue"),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5, color = "darkred")
  ) +
  labs(
    title = "Correlation Matrix of Numeric Features",
    x = "",
    y = ""
  )

# Print the heatmap
print(heatmap_plot)

# Find highly correlated features (correlation > 0.9)
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.9, names = TRUE)

# Remove highly correlated features
if (length(highly_correlated) > 0) {
  cat("Highly correlated features identified and removed:\n")
  print(highly_correlated)
  data <- data %>% select(-all_of(highly_correlated))
} else {
  cat("No highly correlated features found.\n")
}

# --- Reflection on Step 8 ---
cat("\n==== Reflection on Step 8 ====\n")
cat(
  "1. Redundant Features:\n",
  "   - Explicitly removed columns like 'Car_ID' which do not contribute to predictive modeling.\n\n",
  "2. Correlation Matrix:\n",
  "   - Created a heatmap to visualize correlations among numeric features.\n",
  "   - Identified features with high correlation (cutoff: >0.9) and removed them to reduce multicollinearity.\n\n",
  "By performing this step, the dataset is now more refined and ready for modeling.\n"
)

# === Step 9: Export Final Dataset ===
cat("\n==== Step 9: Export Final Dataset ====\n")

# Define the file name for the final dataset
output_file <- "car_prices_preprocessed.csv"  # Descriptive name based on the context

# Export the dataset
tryCatch(
  {
    write.csv(data, output_file, row.names = FALSE)
    cat(sprintf("Final dataset successfully exported to '%s'.\n", output_file))
  },
  error = function(e) {
    cat(sprintf("Error exporting the final dataset: %s\n", e$message))
  }
)

# --- Reflection on Step 9 ---
cat("\n==== Reflection on Step 9 ====\n")
cat(
  "1. The cleaned and preprocessed dataset has been saved as a CSV file named 'car_prices_preprocessed.csv'.\n",
  "2. The file name reflects the dataset's purpose, making it easier to identify and reuse.\n",
  "3. This ensures consistency and reproducibility, providing a high-quality dataset ready for modeling or analysis.\n"
)

# === Modeling and Evaluation ===

# === Step 10: Prepare Data for Modeling ===
cat("\n==== Step 10: Prepare Data for Modeling ====\n")

# Ensure 'price' is numeric
data$price <- as.numeric(data$price)

# --- Split the Data First to Avoid Data Leakage ---
cat("\nSplitting the data into training and testing sets...\n")
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(data$price, p = 0.8, list = FALSE)
data_train <- data[trainIndex, ]
data_test <- data[-trainIndex, ]

cat(sprintf("Training data size: %d rows\n", nrow(data_train)))
cat(sprintf("Testing data size: %d rows\n", nrow(data_test)))

# --- Remove Unnecessary Columns ---
excluded_columns <- c("car_ID", "CarName", "CarBrand")  # IDs and textual data not needed for modeling
data_train <- data_train %>% select(-all_of(excluded_columns))
data_test <- data_test %>% select(-all_of(excluded_columns))

# --- Convert Categorical Variables to Factors ---
categorical_vars <- names(data_train)[sapply(data_train, is.character)]
data_train[categorical_vars] <- lapply(data_train[categorical_vars], as.factor)
data_test[categorical_vars] <- lapply(data_test[categorical_vars], as.factor)

# --- Encode Categorical Variables Using Dummy Variables ---
cat("\nCreating dummy variables for categorical features...\n")
dummies <- dummyVars(" ~ .", data = data_train)
train_data <- data.frame(predict(dummies, newdata = data_train))
test_data <- data.frame(predict(dummies, newdata = data_test))

cat("Dummy variables created.\n")

# --- Ensure the Same Columns in Train and Test Data ---
missing_cols <- setdiff(names(train_data), names(test_data))
for (col in missing_cols) {
  test_data[[col]] <- 0
}
test_data <- test_data[, names(train_data)]

# --- Step 11: Scaling Numeric Features ---
cat("\n==== Step 11: Scaling Numeric Features ====\n")

# Identify numeric columns (excluding the target variable 'price')
numeric_cols <- setdiff(names(train_data)[sapply(train_data, is.numeric)], "price")

# Apply Min-Max Scaling to Numeric Columns using training data parameters
cat("\nApplying Min-Max Scaling to numeric features...\n")
preProcValues <- preProcess(train_data[, numeric_cols], method = c("range"))
train_data[, numeric_cols] <- predict(preProcValues, train_data[, numeric_cols])
test_data[, numeric_cols] <- predict(preProcValues, test_data[, numeric_cols])

cat("Features scaled using Min-Max scaling.\n")

# --- Remove Zero or Near-Zero Variance Predictors ---
nzv <- nearZeroVar(train_data)
if (length(nzv) > 0) {
  cat("\nRemoving near-zero variance predictors:\n")
  print(names(train_data)[nzv])
  train_data <- train_data[, -nzv]
  test_data <- test_data[, -nzv]
}


# --- Step 11: Scaling Numeric Features ---
cat("\n==== Step 11: Scaling Numeric Features ====\n")

# Identify numeric columns (excluding the target variable 'price')
numeric_cols <- setdiff(names(train_data)[sapply(train_data, is.numeric)], "price")

# Apply Min-Max Scaling to Numeric Columns using training data parameters
cat("\nApplying Min-Max Scaling to numeric features...\n")
preProcValues <- preProcess(train_data[, numeric_cols], method = c("range"))
train_data[, numeric_cols] <- predict(preProcValues, train_data[, numeric_cols])
test_data[, numeric_cols] <- predict(preProcValues, test_data[, numeric_cols])

cat("Features scaled using Min-Max scaling.\n")

# Remove Zero or Near-Zero Variance Predictors
nzv <- nearZeroVar(train_data)
if (length(nzv) > 0) {
  cat("\nRemoving near-zero variance predictors:\n")
  print(names(train_data)[nzv])
  train_data <- train_data[, -nzv]
  test_data <- test_data[, -nzv]
}

## ---- Step 12: Build the Linear Regression Model ----
cat("\n==== Step 12: Building the Linear Regression Model ====\n")

# Build the model
linear_model <- lm(price ~ ., data = train_data)

# Model summary
cat("\nModel Summary:\n")
print(summary(linear_model))

## ---- Step 13: Evaluate the Model ----
cat("\n==== Step 13: Evaluating the Model on Test Data ====\n")

# Make predictions
predictions <- predict(linear_model, newdata = test_data)

# Calculate evaluation metrics
rmse <- sqrt(mean((predictions - test_data$price)^2))
r_squared <- 1 - sum((predictions - test_data$price)^2) / sum((mean(test_data$price) - test_data$price)^2)

cat(sprintf("RMSE on Test Data: %.2f\n", rmse))
cat(sprintf("R-squared on Test Data: %.2f\n", r_squared))

## ---- Step 14: Visualize Actual vs Predicted Values ----
cat("\n==== Step 14: Visualizing Actual vs Predicted Values ====\n")

ggplot(data.frame(Actual = test_data$price, Predicted = predictions), aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Predicted Car Prices", x = "Actual Price", y = "Predicted Price") +
  theme_minimal()
## ---- Step 15: Residual Analysis ----
cat("\n==== Step 15: Residual Analysis ====\n")

# Calculate residuals
residuals <- test_data$price - predictions

# Residual Plot
ggplot(data.frame(Predicted = predictions, Residuals = residuals), aes(x = Predicted, y = Residuals)) +
  geom_point(alpha = 0.5, color = "purple") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Predicted Prices", x = "Predicted Price", y = "Residuals") +
  theme_minimal()

# Distribution of Residuals
ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
  geom_histogram(bins = 30, fill = "orange", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Residuals", x = "Residuals", y = "Frequency") +
  theme_minimal()

## ---- Step 16: Feature Importance ----
cat("\n==== Step 16: Feature Importance ====\n")

# Extracting and visualizing feature importance
importance <- summary(linear_model)$coefficients[, "Estimate"]
importance_df <- data.frame(Feature = names(importance), Importance = abs(importance)) %>%
  arrange(desc(Importance))

ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Feature Importance", x = "Features", y = "Coefficient Magnitude") +
  theme_minimal()

## ---- Step 17: Interpretation of Results ----
cat("\n==== Step 17: Interpretation of Results ====\n")

cat("The linear regression model has been built and evaluated after applying necessary data preprocessing steps.\n")
cat(sprintf("RMSE on Test Data: %.2f\n", rmse))
cat(sprintf("R-squared on Test Data: %.2f\n", r_squared))
cat("The model explains a significant portion of the variance in car prices.\n")
cat("Feature importance analysis identifies the most influential variables on car prices.\n")


## ---- Step 18: Interpretation of Results ----
cat("\n==== Step 18: Interpretation of Results ====\n")

cat("The linear regression model has been built and evaluated after scaling numeric features.\n")
cat(sprintf("RMSE on Test Data: %.2f\n", rmse))
cat(sprintf("R-squared on Test Data: %.2f\n", r_squared))
cat("The model explains a significant portion of the variance in car prices.\n")
cat("Feature importance analysis identifies the most influential variables on car prices.\n")
```
